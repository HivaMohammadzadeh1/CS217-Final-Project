\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amsmath}

\title{
  Layer-Adaptive MX Quantization for RLHF on FPGAs: Milestone Report \\
  \vspace{0.5em}
  \small{\normalfont Stanford CS217 - Winter 2026}
}

\author{
  Hiva Zaad (Mohammadzadeh) \\
  Stanford University \\
  \texttt{hiva@stanford.edu} \\
  \And
  Grant Griffith \\
  Stanford University \\
  \texttt{grgriff@stanford.edu} \\
  \And
  Daniel Adkins \\
  Stanford University \\
  \texttt{dadkins@stanford.edu}
}

\begin{document}

\maketitle

\begin{abstract}
We report progress on layer-adaptive MX quantization for RLHF training on FPGAs. In Weeks 1-2, we established the INT8 FPGA baseline using Lab 1 hardware and measured performance on AWS F2 (Xilinx VU9P). Our measurements show 148,656 cycles per 16×16 INT8 matmul with 99.99\% time spent on PCIe data transfer vs 0.01\% on computation. This bottleneck motivates our MX format investigation: MXFP4 offers 50\% bandwidth reduction, projecting 25\% energy savings for RLHF workloads if applied to layers that tolerate aggressive quantization.
\end{abstract}

\section{Progress Summary}

\textbf{Completed (Weeks 1-2):} (1) Repository setup with PyTorch, TRL, and FPGA integration infrastructure; (2) Lab 1 FPGA bitstream deployed and validated on AWS F2 with 0 test errors; (3) 10-iteration performance measurement campaign yielding 148,656 cycles per 16×16 INT8 matmul; (4) Energy calculation framework for RLHF workload projections; (5) RLHF training loop functional with Qwen2.5-0.5B and HH-RLHF dataset.

\textbf{In Progress:} CPU baseline measurements and PyTorch layer sensitivity profiling (Week 3).

\section{FPGA Baseline Performance}

\subsection{Hardware Configuration}
\textbf{Platform:} AWS F2.6xlarge (Xilinx VU9P), 250 MHz clock, 35W power estimate. \textbf{Accelerator:} 16×16 INT8 matrix multiply from Lab 1.

\subsection{Measurements}
We measured FPGA performance over 10 iterations, logging PCIe data transfer cycles (host↔FPGA communication) and compute cycles (on-chip matrix multiplication). Results in Table~\ref{tab:fpga_baseline}.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\ \midrule
Total Cycles/matmul & 148,656 cycles \\
\quad Data Transfer & 148,641 cycles (99.99\%) \\
\quad Computation & 15 cycles (0.01\%) \\
Time/matmul & 594.62 $\mu$s \\
Energy/matmul & 20.81 $\mu$J \\
Throughput & 1,682 ops/sec \\
\bottomrule
\end{tabular}
\caption{FPGA baseline performance (10-iteration average)}
\label{tab:fpga_baseline}
\end{table}

\subsection{Analysis and Implications}

\textbf{Key Finding:} The FPGA compute is extremely fast (15 cycles), but \textbf{PCIe data transfer dominates} (99.99\%). For each 16×16 matmul, the FPGA moves 336 bytes (256B weights + 16B inputs + 64B outputs). Ideal PCIe Gen3 x16 bandwidth ($\sim$12 GB/s) predicts 28 $\mu$s transfer time, but we observe 594.56 $\mu$s due to AXI protocol overhead, host memory copies, and SRAM serialization.

\textbf{MX Format Motivation:} This bottleneck directly motivates compression:
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{MXFP8}: Same 8-bit weight storage as INT8; no bandwidth reduction, but adaptive group scaling may reduce computation energy.
    \item \textbf{MXFP4}: 4-bit weights → 50\% bandwidth reduction → proportional latency/energy savings.
    \item \textbf{Layer-adaptive policies}: Apply MXFP4 to robust layers, MXFP8 to sensitive layers.
\end{itemize}

\textbf{Projected Savings:} If 50\% of layers tolerate MXFP4 without quality loss:
\begin{align*}
\text{Transfer reduction} &= 0.5 \times 50\% = 25\% \text{ fewer cycles} \\
\text{Projected cycles} &= 148,656 \times 0.75 \approx 111,500 \text{ cycles}
\end{align*}
This yields \textbf{25\% energy reduction} for data transfer. Full system savings depend on sensitivity profiling results (Week 3).

\section{RLHF Workload Energy Projections}

For Qwen2.5-0.5B (24 layers, 7 matmuls/layer), each RLHF sample requires:
\begin{align*}
\text{Forward pass:} &\quad 24 \times 7 + 2 = 170 \text{ matmuls} \\
\text{Total (fwd+bwd):} &\quad 170 \times 2 = 340 \text{ matmuls/sample}
\end{align*}

For 50 PPO steps with batch size 8: $50 \times 8 \times 340 = 136,000$ matmuls total.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Workload} & \textbf{Time (min)} & \textbf{Energy (Wh)} \\ \midrule
2 PPO steps & 67.4 & 0.039 \\
10 PPO steps & 337.2 & 0.196 \\
50 PPO steps & 1,686.1 & 0.982 \\
100 PPO steps & 3,372.2 & 1.964 \\
\bottomrule
\end{tabular}
\caption{FPGA energy estimates (INT8 baseline)}
\label{tab:fpga_energy}
\end{table}

\textbf{Note:} Projections assume continuous FPGA use. Host CPU overhead (tokenization, sampling, KL divergence) adds $\sim$20-30\% to end-to-end time, to be measured in integrated experiments.

\textbf{CPU Comparison:} CPU baseline in progress. Expected CPU performance: $\sim$15-20 min, $\sim$0.04-0.05 Wh for 50 steps. FPGA is slower due to PCIe overhead despite lower power (35W vs 150W). Our MX investigation addresses this via compression, batching, and on-chip caching.

\section{Next Steps and Timeline}

\textbf{Week 3 (Feb 24 - Mar 2):}
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{ML Track:} Per-layer sensitivity profiling for Qwen2.5-0.5B. Test MXFP4/MXFP8 on attention, FFN, embeddings. Measure perplexity deltas. Define policies A/B/C/D.
    \item \textbf{HW Track:} Design dual-precision MXFP4/MXFP8 datapath in SystemC. Extend Lab 1 PE with mode selection. Implement group scaling. Verify against PyTorch reference.
    \item \textbf{Integration:} Complete CPU baseline. Prepare matmul offload interface. Build adaptive controller.
\end{itemize}

\textbf{Week 4 (Mar 3-9):} Synthesize MX datapath. Deploy to AWS F2. Integrate offload hook into TRL. Run 10-step smoke test.

\textbf{Weeks 5-6 (Mar 10-21):} Execute experimental matrix (policies A/B/C/D). Measure energy and quality. Generate Pareto curves. Write final report.

\section{Risk Assessment}

\textbf{Risk 1 (Data transfer overhead):} 99.99\% of time is PCIe transfer. \textit{Mitigation:} MXFP4 reduces bandwidth 50\%; batch processing amortizes latency.

\textbf{Risk 2 (Synthesis timing):} MX datapath may fail timing. \textit{Mitigation:} Extensive SystemC simulation (Week 3). Pipeline aggressively if needed.

\textbf{Risk 3 (Aggressive quantization):} Some policies will degrade quality. \textit{Mitigation:} Expected—defines upper bound. Policies A/B/D should succeed.

\section{Key Insights}

The 15-cycle compute vs 148,641-cycle transfer reveals \textbf{memory bandwidth, not compute, limits FPGA RLHF}. This has broader implications: (1) Commercial FPGA accelerators should prioritize compression over raw FLOPS; (2) Hybrid CPU-FPGA systems need co-design to minimize PCIe round-trips; (3) On-chip SRAM is critical.

The data transfer bottleneck strongly favors MXFP4: 50\% bandwidth reduction translates nearly 1:1 to energy savings. The negligible compute cost (15 cycles) means precision loss has minimal performance impact.

\textbf{Hypothesis:} If $\geq$50\% of layers tolerate MXFP4, we expect $\sim$25\% total energy reduction (50\% of the 99.99\% transfer-dominated regime).

\section{Conclusion}

We completed Milestones 1-2, establishing the INT8 FPGA baseline (148,656 cycles/matmul, 99.99\% transfer overhead). Week 3 focuses on sensitivity profiling and MX datapath design to determine if layer-adaptive quantization delivers energy savings without sacrificing RLHF quality. Infrastructure is solid, measurements reproducible, and we remain on track.

\vspace{0.5em}
\noindent\textbf{Repository:} \url{https://github.com/HivaMohammadzadeh1/CS217-Final-Project}

\end{document}
